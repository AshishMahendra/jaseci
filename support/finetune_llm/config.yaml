model:
  hf_dataset: ["chandralegend/map_gen_randomized"]
  hf_model: "HuggingFaceTB/SmolLM-135M"
  output_model: "mtllm-levelgen-smollm-135m"

lora_config:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

training_args:
  max_num_epochs: 2
  learning_rate: 2e-4
  lr_scheduler_type: "cosine"
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  optim: "paged_adamw_32bit"
  save_strategy: "steps"
  save_steps: 500
  logging_steps: 100
  save_total_limit: 2
  num_train_epochs: 2
  max_steps: 1000
  fp16: true

trainer:
  dataset_text_fields: "text"
  max_seq_length: 2048

push_to_hf:
  hf_username: chandralegend